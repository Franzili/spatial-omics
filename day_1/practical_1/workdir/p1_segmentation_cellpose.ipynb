{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELIXIR Spatial Transcriptomics Course\n",
    "### Practical 1c: Segmentation of spatial transcriptomics data using `cellpose`\n",
    "Date: 2025-01-22\n",
    "\n",
    "Author(s): Rasool Saghaleyni\n",
    "\n",
    "Author(s) email: rasool.saghaleyni@scilifelab.se\n",
    "\n",
    "⚠️ Note: The proper environment for this notebook is `p1_segmentation_cellpose`. It can be activated by selecting the kernel in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cellpose import models, io, plot\n",
    "from tifffile import imread\n",
    "import os\n",
    "import tifffile\n",
    "from cellpose import plot\n",
    "import shapely.geometry as geometry\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import translate, scale\n",
    "from shapely.errors import TopologicalError\n",
    "from rasterio import features\n",
    "from sklearn.metrics import jaccard_score\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "\n",
    "# Set up plotting aesthetics\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and inspecting the morphology image is a foundational step in the analysis. Understanding the image's dimensions helps guide downstream processing, including selecting channels or slices for segmentation and defining an ROI if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_path = '/data/spatial_workshop/day1/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/morphology.ome.tif'\n",
    "image = tifffile.imread(image_data_path)\n",
    "print(f\"Image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we retrieve and inspect the metadata embedded within the morphology image file. Opening the file with `tifffile.TiffFile` allows us to access not only the pixel data but also any associated metadata stored in OME (Open Microscopy Environment) format. By using a context manager (`with` statement), we ensure that the file is properly handled, meaning it will close automatically once we’re done, helping to avoid potential file-handling errors.\n",
    "\n",
    "The OME metadata contains crucial information about the image acquisition settings. Extracting it with `ome_metadata = tif.ome_metadata` provides us with details about the microscope settings, pixel size, and other experimental parameters. This metadata appears in XML format, which is printed for review. Examining this data is essential to understand the spatial resolution of the image, enabling us to relate image coordinates to real-world units, such as micrometers. Knowing these specifics is key for aligning segmentation results accurately with the spatial features observed in the morphology image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tifffile.TiffFile(image_data_path) as tif:\n",
    "    ome_metadata = tif.ome_metadata\n",
    "    print(ome_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets preview the transcripts data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptomics_data_path = '/data/spatial_workshop/day1/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/transcripts.csv.gz'\n",
    "data = pd.read_csv(transcriptomics_data_path, compression='gzip')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are checking and handling the dimensionality of the morphology image to extract a usable 2D channel for segmentation and analysis. To start, `print(f\"Image dimensions: {image.ndim}\")` reveals the number of dimensions in the image array. Images acquired from microscopy can have multiple dimensions, often representing different z-slices, time points, or channels (e.g., specific fluorescent stains). Knowing the exact number of dimensions is essential for understanding the structure of the data and selecting the specific layer or channel needed for downstream tasks.\n",
    "\n",
    "We then use conditional statements to select the appropriate 2D plane. If the image has five dimensions—typically representing time, Z (depth), channels, height, and width—we select the first time point, Z-slice, and channel to reduce it to 2D. Similarly, for four-dimensional images (likely Z, channels, height, and width), we choose the first Z-slice and channel. In the case of three-dimensional images, we assume they represent channels, height, and width, and extract the first channel. Finally, if the image is already 2D, we simply assign it to image_channel without further modification.\n",
    "\n",
    "This step ensures that we have a consistent, interpretable 2D array (image_channel) for the following analysis. By isolating a single plane or channel, we simplify the data, making it easier to overlay segmentations or spatial features without the added complexity of multiple dimensions. This also ensures that our chosen channel represents the tissue morphology effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image dimensions: {image.ndim}\")\n",
    "if image.ndim == 5:\n",
    "    # Example shape: (Time, Z, Channels, Height, Width)\n",
    "    # Select the first time point, z-slice, and channel\n",
    "    image_channel = image[0, 0, 0, :, :]\n",
    "elif image.ndim == 4:\n",
    "    # Example shape: (Z, Channels, Height, Width)\n",
    "    image_channel = image[0, 0, :, :]\n",
    "elif image.ndim == 3:\n",
    "    # Example shape: (Channels, Height, Width)\n",
    "    image_channel = image[0, :, :]\n",
    "else:\n",
    "    # Already a 2D image\n",
    "    image_channel = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channel = np.max(image, axis=0)\n",
    "image_channel = image_channel.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_channel)\n",
    "plt.title('Selected Image for Segmentation')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original image is too big here we define a region of interest (ROI) within the larger image, focusing on a smaller area for more efficient and targeted analysis. This is particularly useful for high-resolution images where analyzing the entire field of view might be computationally intensive.\n",
    "\n",
    "We begin by setting a scale_factor, which controls the size of the ROI as a fraction of the full image dimensions. Here, `scale_factor = 0.05` means that the ROI will cover 5% of the original image's width and height. Adjusting this factor allows flexibility in focusing on larger or smaller portions of the image, depending on the needs of the analysis.\n",
    "\n",
    "Using `image_channel.shape`, we extract the height and width of the full image. Then, by multiplying these dimensions by `scale_factor`, we calculate the width and height of the ROI (`roi_width` and `roi_height`). Converting these values to integers ensures that they’re compatible with image indexing.\n",
    "\n",
    "Finally, we make an optional adjustment to ensure that the ROI dimensions are even numbers, which can simplify image processing tasks. We do this by reducing `roi_width` and `roi_height` by 1 if they are odd, using modulo operations. This adjustment helps avoid issues when working with certain algorithms that may require even-numbered dimensions, ensuring that the ROI dimensions are compatible with a range of image processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.05\n",
    "image_height, image_width = image_channel.shape\n",
    "\n",
    "# roi\n",
    "roi_width = int(image_width * scale_factor)\n",
    "roi_height = int(image_height * scale_factor)\n",
    "roi_width -= roi_width % 2\n",
    "roi_height -= roi_height % 2\n",
    "\n",
    "#calculate the starting and ending coordinates\n",
    "x_center = image_width // 2\n",
    "y_center = image_height // 2\n",
    "\n",
    "x_start = x_center - roi_width // 2\n",
    "x_end = x_center + roi_width // 2\n",
    "y_start = y_center - roi_height // 2\n",
    "y_end = y_center + roi_height // 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the ROI from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_image = image_channel[y_start:y_end, x_start:x_end]\n",
    "# dimensions\n",
    "print(f\"ROI image shape: {roi_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(roi_image)\n",
    "plt.title('ROI Image for Segmentation')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we map the ROI pixel coordinates back to real-world units (micrometers) and filter the spatial transcriptomics data to include only the transcripts within the ROI. This enables precise alignment of the transcript data with the selected region in the image.\n",
    "\n",
    "We start by defining scaling factors for converting image pixels to micrometers. Here, `x_scale` and `y_scale` represent the conversion rate based on the pixel size provided in the image metadata: each micrometer contains approximately 4.7 pixels (1 / 0.2125). This conversion allows us to translate pixel coordinates into micrometer units, which are required for comparing and aligning data across different scales.\n",
    "\n",
    "Using these scaling factors, we calculate the boundaries of the ROI in micrometers. For each dimension, `x_start`, `x_end`, `y_start`, and `y_end` (which are pixel coordinates from the original image), we divide by the scaling factor to obtain the corresponding boundaries in micrometers: `x_start_um`, `x_end_um`, `y_start_um`, and `y_end_um`. This step ensures that our ROI is defined consistently in both pixel and physical units.\n",
    "\n",
    "Next, we filter the transcriptomics data to include only the transcripts located within the ROI. We use conditional filtering on the `x_location` and `y_location` columns of the data DataFrame, retaining only the transcripts whose coordinates fall within the calculated micrometer boundaries. The result is stored in roi_data, which represents the subset of transcripts that reside within our chosen ROI.\n",
    "\n",
    "Finally, by printing `len(roi_data)`, we get a quick count of the transcripts within the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the scaling factors from before\n",
    "x_scale = 1 / 0.2125  # pixels per µm\n",
    "y_scale = 1 / 0.2125  # pixels per µm\n",
    "\n",
    "#roi boundaries in micrometers\n",
    "x_start_um = x_start / x_scale\n",
    "x_end_um = x_end / x_scale\n",
    "y_start_um = y_start / y_scale\n",
    "y_end_um = y_end / y_scale\n",
    "\n",
    "#filter transcripts within the ROI boundaries\n",
    "roi_data = data[\n",
    "    (data['x_location'] >= x_start_um) &\n",
    "    (data['x_location'] < x_end_um) &\n",
    "    (data['y_location'] >= y_start_um) &\n",
    "    (data['y_location'] < y_end_um)\n",
    "].copy()\n",
    "print(f\"Number of transcripts in ROI: {len(roi_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should subtract `x_start_um` from each transcript’s `x_location` and `y_start_um` from each `y_location` in `roi_data`. By doing so, we create new columns, `x_location_roi` and `y_location_roi`, that represent each transcript’s position relative to the top-left corner of the ROI rather than the full image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_data['x_location_roi'] = roi_data['x_location'] - x_start_um\n",
    "roi_data['y_location_roi'] = roi_data['y_location'] - y_start_um"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally set up the Cellpose segmentation model to identify cells within the ROI. Cellpose is a versatile deep learning-based tool commonly used for cell segmentation, especially on fluorescence and cytoplasmic images. Here, we are preparing the model for use in the analysis.\n",
    "\n",
    "First, we import the models module from the cellpose package, which provides access to pre-trained Cellpose models. Next, we initialize a model instance using `models.Cellpose()`. By setting `gpu=False`, we specify that the model will run on the CPU. This is useful if GPU resources are unavailable, though using a GPU can speed up the segmentation process if it is an option.\n",
    "\n",
    "We also set `model_type='cyto'`, indicating that the model should use Cellpose’s pre-trained “cyto” (cytoplasm) model, which is optimized for identifying cell boundaries in images with visible cell structures. This choice is typically well-suited for images showing cell cytoplasm, though Cellpose offers other model types, like “nuclei,” if our focus were solely on nuclear segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "model = models.Cellpose(gpu=False, model_type='cyto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we estimate the average cell diameter in pixels and then use Cellpose to perform cell segmentation on the ROI.\n",
    "\n",
    "We start by setting `cell_diameter_um` to an estimated cell diameter in micrometers, which is based on biological knowledge of cell sizes in the specific tissue or sample type. Here, we use 10 micrometers as the mouse cell daimeter for mouse brains cells is estrimated 7-10 micrometers, but this value can be adjusted based on the specific dataset.\n",
    "\n",
    "To convert this estimate into pixel units, we multiply `cell_diameter_um` by the scaling factor `x_scale` (pixels per micrometer), calculated previously. This results in `cell_diameter_pixels`, an approximation of the cell diameter in the pixel space of the image. By converting the diameter to pixels, we ensure that Cellpose can interpret the size parameter relative to the image’s resolution.\n",
    "\n",
    "Next, we run the Cellpose model on `roi_image`, the 2D image extracted from the ROI. The model’s eval() function applies the segmentation model to the image, using `diameter=cell_diameter_pixels` to guide the segmentation scale. The channels=[0, 0] parameter specifies that the image is grayscale; both entries as 0 indicate that there is a single channel for both input and detection purposes.\n",
    "\n",
    "The eval() function returns several outputs:\n",
    "\n",
    "- `masks`: a labeled mask array where each detected cell has a unique identifier,\n",
    "  \n",
    "- `flows`: which provides information about cell boundary flows,\n",
    "  \n",
    "- `styles`: representing style vectors for detected objects, and\n",
    "  \n",
    "- `diams`: the diameter used in the model (helpful if it has been automatically adjusted).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_diameter_um = 10  # µm\n",
    "cell_diameter_pixels = cell_diameter_um * x_scale\n",
    "print(f\"Estimated cell diameter in pixels: {cell_diameter_pixels}\")\n",
    "#run segmentation\n",
    "masks, flows, styles, diams = model.eval(\n",
    "    roi_image,\n",
    "    diameter=cell_diameter_pixels,\n",
    "    channels=[0, 0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of cells detected in ROI: {masks.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the transcript coordinates within the ROI from micrometers to pixel indices, preparing them for alignment with the segmentation mask.\n",
    "\n",
    "First, we extract the x- and y-coordinates in micrometers from `roi_data`, which represents transcript locations relative to the top-left corner of the ROI. These coordinates are stored in `x_coords_um` and `y_coords_um`, making it easy to work directly with arrays of positions.\n",
    "\n",
    "To map these positions into the pixel space of `roi_image`, we multiply each coordinate by the scaling factor (`x_scale` and `y_scale`) previously defined. This scaling factor converts micrometers into pixel units, allowing us to obtain `x_indices` and `y_indices` the pixel indices that match the resolution of the segmentation mask.\n",
    "\n",
    "By converting coordinates to pixel indices, we can precisely locate each transcript in the context of the segmented cells within the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coords_um = roi_data['x_location_roi'].values\n",
    "y_coords_um = roi_data['y_location_roi'].values\n",
    "\n",
    "#pixel indices\n",
    "x_indices = (x_coords_um * x_scale).astype(int)\n",
    "y_indices = (y_coords_um * y_scale).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roi image dimensions\n",
    "roi_height, roi_width = roi_image.shape\n",
    "\n",
    "#indices\n",
    "x_indices = np.clip(x_indices, 0, roi_width - 1)\n",
    "y_indices = np.clip(y_indices, 0, roi_height - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we assign each transcript to a segmented cell based on its pixel coordinates, linking gene expression data to specific cells within the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell labels for each transcript\n",
    "cell_labels = masks[y_indices, x_indices]\n",
    "#Add cell labels to the data\n",
    "roi_data['cellpose_cell_id'] = cell_labels\n",
    "\n",
    "#preview\n",
    "print(roi_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only transcripts assigned to a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_data = roi_data[roi_data['cellpose_cell_id'] > 0].copy()\n",
    "print(f\"Total transcripts in ROI: {len(roi_data)}\")\n",
    "print(f\"Assigned transcripts: {len(assigned_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we visualize the results of the Cellpose segmentation overlayed on the ROI image, allowing us to inspect how well the model identified individual cells in the selected region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plot.show_segmentation(fig, roi_image, masks, flows[0])\n",
    "plt.title('Cellpose Segmentation on ROI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by cell and gene to get expression counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_per_cell = assigned_data.groupby(['cellpose_cell_id', 'feature_name']).size().reset_index(name='count')\n",
    "expression_matrix = expression_per_cell.pivot(index='cellpose_cell_id', columns='feature_name', values='count').fillna(0)\n",
    "print(expression_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the locations of transcripts overlaid on the ROI image, specifically highlighting those that have been assigned to segmented cells. This helps us see how transcript data aligns with the detected cell boundaries within the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(roi_image, cmap='gray')\n",
    "plt.scatter(\n",
    "    x_indices[roi_data['cellpose_cell_id'] > 0],\n",
    "    y_indices[roi_data['cellpose_cell_id'] > 0],\n",
    "    c='red', s=5, label='Transcripts'\n",
    ")\n",
    "plt.title('Transcripts Mapped to Segmented Cells')\n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the expression of a specific gene across the detected cells, providing insights into the spatial distribution of gene expression within the ROI. This visualization can reveal patterns of gene expression, such as high expression in specific cell types or regions, helping to interpret the biological significance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_of_interest = 'Cst3'\n",
    "\n",
    "if gene_of_interest in expression_matrix.columns:\n",
    "    cell_ids = expression_matrix.index.values\n",
    "    expression_values = expression_matrix[gene_of_interest].values\n",
    "\n",
    "    #get centroids of cells\n",
    "    from skimage.measure import regionprops\n",
    "    properties = regionprops(masks)\n",
    "    centroids = np.array([prop.centroid for prop in properties])\n",
    "    cell_labels = np.array([prop.label for prop in properties])\n",
    "\n",
    "    #create a mapping from cell label to centroid\n",
    "    centroid_dict = {label: centroid for label, centroid in zip(cell_labels, centroids)}\n",
    "\n",
    "    #get centroids for the cells in expression_matrix\n",
    "    cell_centroids = np.array([centroid_dict.get(cell_id, (np.nan, np.nan)) for cell_id in cell_ids])\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(roi_image, cmap='gray')\n",
    "    plt.scatter(\n",
    "        cell_centroids[:, 1],  # x-coordinates\n",
    "        cell_centroids[:, 0],  # y-coordinates\n",
    "        c=expression_values,\n",
    "        cmap='viridis',\n",
    "        s=50,\n",
    "        edgecolors='k',\n",
    "        label=f'Expression of {gene_of_interest}'\n",
    "    )\n",
    "    plt.title(f'Expression of {gene_of_interest}')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(label='Expression Level')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"{gene_of_interest} not found in expression matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to compare the cellpose segmentation with 10x segmentation, we can use the Jaccard index to quantify the similarity between the two segmentation masks. The Jaccard index, also known as the intersection-over-union (IoU), measures the overlap between two sets by dividing the size of their intersection by the size of their union. In the context of segmentation masks, the Jaccard index provides a measure of how well two masks align, with values closer to 1 indicating greater similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_data_path = '../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cells.csv'\n",
    "cells_data = pd.read_csv(cells_data_path)\n",
    "print(cells_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries_path = '../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cell_boundaries.csv.gz'  \n",
    "cell_boundaries = pd.read_csv(cell_boundaries_path)\n",
    "print(cell_boundaries.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to extract the region of intrest from the 10x segmentation mask, then we need to resize the 10x segmentation mask to the same size as the cellpose segmentation mask, then we can calculate the Jaccard index between the two masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.05\n",
    "image_height, image_width = image_channel.shape\n",
    "roi_width = int(image_width * scale_factor)\n",
    "roi_height = int(image_height * scale_factor)\n",
    "\n",
    "roi_width -= roi_width % 2\n",
    "roi_height -= roi_height % 2\n",
    "\n",
    "x_center = image_width // 2\n",
    "y_center = image_height // 2\n",
    "\n",
    "x_start = x_center - roi_width // 2\n",
    "x_end = x_center + roi_width // 2\n",
    "y_start = y_center - roi_height // 2\n",
    "y_end = y_center + roi_height // 2\n",
    "\n",
    "#cnvert pixel coordinates to micrometers\n",
    "x_scale = 1 / 0.2125  \n",
    "y_scale = 1 / 0.2125\n",
    "\n",
    "x_start_um = x_start / x_scale\n",
    "x_end_um = x_end / x_scale\n",
    "y_start_um = y_start / y_scale\n",
    "y_end_um = y_end / y_scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter cells whose centroids are within the ROI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_in_roi = cells_data[\n",
    "    (cells_data['x_centroid'] >= x_start_um) &\n",
    "    (cells_data['x_centroid'] < x_end_um) &\n",
    "    (cells_data['y_centroid'] >= y_start_um) &\n",
    "    (cells_data['y_centroid'] < y_end_um)\n",
    "].copy()\n",
    "\n",
    "print(f\"Number of cells in ROI from original segmentation: {len(cells_in_roi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter cell boundaries for cells in ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries_in_roi = cell_boundaries[cell_boundaries['cell_id'].isin(cells_in_roi['cell_id'])].copy()\n",
    "cell_polygons = {}\n",
    "for cell_id, group in cell_boundaries_in_roi.groupby('cell_id'):\n",
    "    x_coords = group['vertex_x'].values\n",
    "    y_coords = group['vertex_y'].values\n",
    "    coords = list(zip(x_coords, y_coords))\n",
    "    try:\n",
    "        polygon = Polygon(coords)\n",
    "        if not polygon.is_valid:\n",
    "            # Attempt to fix invalid polygons\n",
    "            polygon = polygon.buffer(0)\n",
    "        cell_polygons[cell_id] = polygon\n",
    "    except TopologicalError as e:\n",
    "        print(f\"Could not create polygon for cell {cell_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a function to transform geometries to pixel coordinates\n",
    "def geometry_to_pixel_coords(geometry):\n",
    "    #shift geometry to ROI coordinates (subtract ROI origin in micrometers)\n",
    "    geometry_shifted = translate(geometry, xoff=-x_start_um, yoff=-y_start_um)\n",
    "    #scale geometry from micrometers to pixels\n",
    "    geometry_scaled = scale(geometry_shifted, xfact=x_scale, yfact=y_scale, origin=(0, 0))\n",
    "    return geometry_scaled\n",
    "\n",
    "# now appply transformation to all cell polygons\n",
    "cell_polygons_px = {cell_id: geometry_to_pixel_coords(geom) for cell_id, geom in cell_polygons.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map cell_id strings to integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id_to_label = {cell_id: idx+1 for idx, cell_id in enumerate(cell_polygons_px.keys())}\n",
    "label_to_cell_id = {idx+1: cell_id for idx, cell_id in enumerate(cell_polygons_px.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare shapes for rasterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [\n",
    "    (geom, cell_id_to_label[cell_id])\n",
    "    for cell_id, geom in cell_polygons_px.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty mask and rasterize the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_masks = np.zeros_like(roi_image, dtype=np.uint16)\n",
    "original_masks = features.rasterize(\n",
    "    shapes,\n",
    "    out_shape=original_masks.shape,\n",
    "    fill=0,\n",
    "    all_touched=True,\n",
    "    dtype=np.uint16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the cellpose segmentation with 10x segmentation side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "#Cellpose \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roi_image, cmap='gray')\n",
    "plt.imshow(masks, alpha=0.5, cmap='jet')\n",
    "plt.title('Cellpose Segmentation')\n",
    "plt.axis('off')\n",
    "\n",
    "#Original \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(roi_image, cmap='gray')\n",
    "plt.imshow(original_masks, alpha=0.5, cmap='jet')\n",
    "plt.title('Original 10x Segmentation')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can overlay the cellpose segmentation on the 10x segmentation to see how well they align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we overlay both masks\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(roi_image, cmap='gray')\n",
    "plt.imshow((original_masks > 0).astype(int), cmap='Blues', alpha=0.5, label='Original')\n",
    "plt.imshow((masks > 0).astype(int), cmap='Reds', alpha=0.5, label='Cellpose')\n",
    "plt.title('Overlay of Segmentations')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert masks to binary masks (cells vs background)\n",
    "cellpose_mask_binary = (masks > 0).astype(int)\n",
    "original_mask_binary = (original_masks > 0).astype(int)\n",
    "\n",
    "#flatten the masks for metric computation\n",
    "cellpose_mask_flat = cellpose_mask_binary.flatten()\n",
    "original_mask_flat = original_mask_binary.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jaccard = jaccard_score(original_mask_flat, cellpose_mask_flat)\n",
    "print(f'Jaccard Index: {jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    sum_union = np.sum(y_true) + np.sum(y_pred)\n",
    "    dice = 2 * intersection / sum_union\n",
    "    return dice\n",
    "\n",
    "dice = dice_coefficient(original_mask_flat, cellpose_mask_flat)\n",
    "print(f'Dice Coefficient: {dice:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellpose_cell_count = masks.max()\n",
    "original_cell_count = original_masks.max()\n",
    "\n",
    "print(f\"Number of cells detected by Cellpose: {cellpose_cell_count}\")\n",
    "print(f\"Number of cells in original segmentation: {original_cell_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cellpose cell areas\n",
    "cellpose_props = regionprops_table(masks, properties=['area'])\n",
    "cellpose_areas = pd.DataFrame(cellpose_props)\n",
    "cellpose_areas['method'] = 'Cellpose'\n",
    "\n",
    "#original cell areas\n",
    "original_props = regionprops_table(original_masks, properties=['area'])\n",
    "original_areas = pd.DataFrame(original_props)\n",
    "original_areas['method'] = 'Original'\n",
    "\n",
    "#combine data\n",
    "areas_df = pd.concat([cellpose_areas, original_areas], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare cell sizes between the two segmentation methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=areas_df, x='area', hue='method', common_norm=False)\n",
    "plt.xlabel('Cell Area (pixels)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Cell Size Distribution Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the results to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite('../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cellpose/cellpose_masks_roi.tif', masks.astype(np.uint16))\n",
    "tifffile.imwrite('../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cellpose/original_masks_roi.tif', original_masks.astype(np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'Metric': ['Jaccard Index', 'Dice Coefficient'],\n",
    "    'Value': [jaccard, dice]\n",
    "})\n",
    "metrics.to_csv('../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cellpose/segmentation_comparison_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_df.to_csv('../data/Xenium_V1_FFPE_TgCRND8_17_9_months_outs/cellpose/cell_size_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_segmentation_cellpose",
   "language": "python",
   "name": "p1_segmentation_cellpose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
